#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""RAT QuickDB Python MySQL ç¼“å­˜æ€§èƒ½å¯¹æ¯”ç¤ºä¾‹

æœ¬ç¤ºä¾‹å¯¹æ¯”å¯ç”¨ç¼“å­˜å’Œæœªå¯ç”¨ç¼“å­˜çš„MySQLæ•°æ®åº“æ“ä½œæ€§èƒ½å·®å¼‚
ä½¿ç”¨ MySQL æ•°æ®åº“è¿›è¡Œæµ‹è¯•

åŸºäº MongoDB ç‰ˆæœ¬çš„ç¼“å­˜æ€§èƒ½å¯¹æ¯”ç¤ºä¾‹æ”¹å†™ä¸º MySQL ç‰ˆæœ¬
"""

import json
import time
import os
import shutil
from datetime import datetime
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass

# å¯¼å…¥ä¼˜é›…å…³é—­æœºåˆ¶
from graceful_shutdown import GracefulShutdownMixin, ShutdownConfig, with_graceful_shutdown

# å…¨å±€å˜é‡ç”¨äºä¼˜é›…å…³é—­
import signal
import threading
shutdown_requested = False
test_instance = None
shutdown_lock = threading.Lock()
shutdown_timeout = 15  # å¼ºåˆ¶é€€å‡ºè¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰

def force_exit():
    """å¼ºåˆ¶é€€å‡ºå‡½æ•°"""
    print(f"âš ï¸ ä¼˜é›…å…³é—­è¶…æ—¶ï¼ˆ{shutdown_timeout}ç§’ï¼‰ï¼Œå¼ºåˆ¶é€€å‡ºç¨‹åº")
    import os
    os._exit(1)

def signal_handler(signum, frame):
    """ä¿¡å·å¤„ç†å™¨"""
    global shutdown_requested, test_instance
    
    with shutdown_lock:
        if shutdown_requested:
            print(f"\nğŸ›‘ å†æ¬¡æ”¶åˆ°ä¿¡å· {signum}ï¼Œå¼ºåˆ¶é€€å‡º...")
            force_exit()
            return
        
        shutdown_requested = True
        print(f"\nğŸ›‘ æ”¶åˆ°ä¿¡å· {signum}ï¼Œå¼€å§‹ä¼˜é›…å…³é—­...")
        
        # å¯åŠ¨å¼ºåˆ¶é€€å‡ºå®šæ—¶å™¨
        timer = threading.Timer(shutdown_timeout, force_exit)
        timer.daemon = True
        timer.start()
        
        if test_instance:
            try:
                print("ğŸ”„ æ­£åœ¨æ¸…ç†èµ„æº...")
                test_instance.shutdown()
                timer.cancel()  # å–æ¶ˆå¼ºåˆ¶é€€å‡ºå®šæ—¶å™¨
                print("ğŸ‘‹ ç¨‹åºå·²ä¼˜é›…å…³é—­")
                import sys
                sys.exit(0)
            except Exception as e:
                print(f"âš ï¸ å…³é—­è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
                timer.cancel()
                force_exit()
        else:
            timer.cancel()
            print("ğŸ‘‹ ç¨‹åºå·²é€€å‡º")
            import sys
            sys.exit(0)

try:
    import rat_quickdb_py
    from rat_quickdb_py import (
        create_db_queue_bridge,
        get_version,
        get_info,
        get_name,
        PyCacheConfig,
        PyL1CacheConfig,
        PyL2CacheConfig,
        PyTtlConfig,
        PyCompressionConfig,
    )
except ImportError as e:
    print(f"é”™è¯¯ï¼šæ— æ³•å¯¼å…¥ rat_quickdb_py æ¨¡å—: {e}")
    print("è¯·ç¡®ä¿å·²æ­£ç¡®å®‰è£… rat-quickdb-py åŒ…")
    print("å®‰è£…å‘½ä»¤ï¼šmaturin develop")
    exit(1)


@dataclass
class TestUser:
    """æµ‹è¯•ç”¨æˆ·æ•°æ®ç»“æ„"""
    id: int
    name: str
    email: str
    age: int
    city: str
    created_at: str
    
    @classmethod
    def new(cls, user_id: int, name: str, email: str, age: int, city: str) -> 'TestUser':
        return cls(
            id=user_id,
            name=name,
            email=email,
            age=age,
            city=city,
            created_at=datetime.utcnow().isoformat() + "Z"
        )
    
    def to_json(self) -> str:
        """è½¬æ¢ä¸ºJSONå­—ç¬¦ä¸²ï¼ˆä¸åŒ…å«idï¼Œè®©MySQLè‡ªåŠ¨ç”Ÿæˆï¼‰"""
        return json.dumps({
            "name": self.name,
            "email": self.email,
            "age": self.age,
            "city": self.city,
            "created_at": self.created_at
        })


@dataclass
class PerformanceResult:
    """æ€§èƒ½æµ‹è¯•ç»“æœ"""
    operation: str
    with_cache: float  # æ¯«ç§’
    without_cache: float  # æ¯«ç§’
    improvement_ratio: float
    cache_hit_rate: Optional[float] = None
    
    @classmethod
    def new(cls, operation: str, with_cache: float, without_cache: float) -> 'PerformanceResult':
        improvement_ratio = without_cache / with_cache if with_cache > 0 else 1.0
        return cls(
            operation=operation,
            with_cache=with_cache,
            without_cache=without_cache,
            improvement_ratio=improvement_ratio
        )
    
    def with_cache_hit_rate(self, hit_rate: float) -> 'PerformanceResult':
        self.cache_hit_rate = hit_rate
        return self


class MySqlCachePerformanceTest(GracefulShutdownMixin):
    """MySQLç¼“å­˜æ€§èƒ½å¯¹æ¯”æµ‹è¯•"""
    
    def __init__(self):
        # åˆå§‹åŒ–ä¼˜é›…å…³é—­æœºåˆ¶ï¼Œå‡å°‘è¶…æ—¶æ—¶é—´é˜²æ­¢æ— é™ç­‰å¾…
        super().__init__(ShutdownConfig(
            shutdown_timeout=10,  # å‡å°‘å…³é—­è¶…æ—¶æ—¶é—´åˆ°10ç§’
            verbose_logging=True,
            auto_cleanup_on_exit=True
        ))
        
        self.bridge = None
        self.results: List[PerformanceResult] = []
        self.test_data_dir = "./test_data"
        # ä½¿ç”¨æ—¶é—´æˆ³ä½œä¸ºè¡¨ååç¼€ï¼Œé¿å…é‡å¤
        timestamp = int(time.time() * 1000)
        self.table_name = f"test_users_{timestamp}"
        
        # æ³¨å†Œä¸´æ—¶ç›®å½•
        self.add_temp_dir(self.test_data_dir)
    
    def _cleanup_existing_tables(self):
        """æ¸…ç†ç°æœ‰çš„æµ‹è¯•è¡¨"""
        print("ğŸ§¹ æ¸…ç†ç°æœ‰çš„æµ‹è¯•è¡¨...")
        try:
            # åˆ›å»ºä¸´æ—¶æ¡¥æ¥å™¨è¿›è¡Œæ¸…ç†
            temp_bridge = create_db_queue_bridge()
            
            # æ·»åŠ MySQLæ•°æ®åº“è¿æ¥
            result = temp_bridge.add_mysql_database(
                alias="mysql_cleanup",
                host="172.16.0.21",
                port=3306,
                database="testdb",
                username="testdb",
                password="yash2vCiBA&B#h$#i&gb@IGSTh&cP#QC^",
                max_connections=5,
                min_connections=1,
                connection_timeout=10,
                idle_timeout=300,
                max_lifetime=600
            )
            
            result_data = json.loads(result)
            if result_data.get("success"):
                # åˆ é™¤æµ‹è¯•è¡¨ä¸­çš„æ•°æ®
                tables_to_clean = ["test_users", "users", "performance_test", self.table_name]
                for table in tables_to_clean:
                    try:
                        temp_bridge.drop_table(table, "mysql_cleanup")
                        print(f"âœ… å·²æ¸…ç†è¡¨: {table}")
                    except Exception as e:
                        print(f"âš ï¸ æ¸…ç†è¡¨ {table} æ—¶å‡ºé”™: {e}")
            else:
                print(f"âš ï¸ æ— æ³•è¿æ¥åˆ°MySQLè¿›è¡Œæ¸…ç†: {result_data.get('error')}")
                
        except Exception as e:
            print(f"âš ï¸ æ¸…ç†è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
    
    def initialize(self) -> bool:
        """åˆå§‹åŒ–æµ‹è¯•ç¯å¢ƒ"""
        print("ğŸš€ åˆå§‹åŒ–MySQLç¼“å­˜æ€§èƒ½å¯¹æ¯”æµ‹è¯•ç¯å¢ƒ...")
        
        try:
            # åˆ›å»ºæµ‹è¯•æ•°æ®ç›®å½•
            os.makedirs(self.test_data_dir, exist_ok=True)
            
            # åˆ›å»ºæ•°æ®åº“é˜Ÿåˆ—æ¡¥æ¥å™¨
            self.bridge = create_db_queue_bridge()
            self.add_database_connection(self.bridge)
            
            # æ¸…ç†ç°æœ‰çš„æµ‹è¯•è¡¨
            self._cleanup_existing_tables()
            
            # æ·»åŠ å¸¦ç¼“å­˜çš„MySQLæ•°æ®åº“
            self._add_cached_mysql_database()
            
            # æ·»åŠ ä¸å¸¦ç¼“å­˜çš„MySQLæ•°æ®åº“
            self._add_non_cached_mysql_database()
            
            print("âœ… æµ‹è¯•ç¯å¢ƒåˆå§‹åŒ–å®Œæˆ")
            return True
            
        except Exception as e:
            print(f"âŒ æµ‹è¯•ç¯å¢ƒåˆå§‹åŒ–å¤±è´¥: {e}")
            return False
    
    def _create_cached_config(self) -> PyCacheConfig:
        """åˆ›å»ºå¸¦ç¼“å­˜çš„é…ç½®ï¼ˆæ€§èƒ½ä¼˜åŒ–ç‰ˆæœ¬ï¼‰"""
        cache_config = PyCacheConfig()
        cache_config.enable()
        cache_config.strategy = "lru"
        
        # L1ç¼“å­˜é…ç½® - é€‚é…ç³»ç»Ÿå†…å­˜é™åˆ¶
        l1_config = PyL1CacheConfig(1000)  # 1000æ¡è®°å½•
        l1_config.max_memory_mb = 100  # 100MBå†…å­˜ï¼Œé€‚é…ç³»ç»Ÿé™åˆ¶
        l1_config.enable_stats = False  # ç¦ç”¨ç»Ÿè®¡ä»¥å‡å°‘å¼€é”€
        cache_config.l1_config = l1_config
        
        # L2ç¼“å­˜é…ç½® - åˆç†çš„ç£ç›˜å®¹é‡é…ç½®
        l2_config = PyL2CacheConfig(f"{self.test_data_dir}/mysql_cache_test")
        l2_config.max_disk_mb = 500  # 500MBç£ç›˜ç©ºé—´
        l2_config.compression_level = 6  # ä¸­ç­‰å‹ç¼©çº§åˆ«
        l2_config.enable_wal = True  # å¯ç”¨WAL
        l2_config.clear_on_startup = False  # å¯åŠ¨æ—¶ä¸æ¸…ç©ºç¼“å­˜ç›®å½•
        cache_config.l2_config = l2_config
        
        # TTLé…ç½® - å»¶é•¿ç¼“å­˜æ—¶é—´ç¡®ä¿æµ‹è¯•æœŸé—´ä¸è¿‡æœŸ
        ttl_config = PyTtlConfig(300)  # 5åˆ†é’ŸTTL
        ttl_config.max_ttl_secs = 3600  # 1å°æ—¶æœ€å¤§TTL
        ttl_config.check_interval_secs = 60  # 1åˆ†é’Ÿæ£€æŸ¥é—´éš”
        cache_config.ttl_config = ttl_config
        
        # å‹ç¼©é…ç½® - å¯ç”¨å‹ç¼©
        compression_config = PyCompressionConfig("zstd")
        compression_config.enabled = True  # å¯ç”¨å‹ç¼©
        compression_config.threshold_bytes = 1024
        cache_config.compression_config = compression_config
        
        print("  ğŸ“Š ç¼“å­˜é…ç½®: L1(1000æ¡/100MB) + L2(500MB) + TTL(5åˆ†é’Ÿ) + ZSTDå‹ç¼©")
        return cache_config
    
    def _add_cached_mysql_database(self):
        """æ·»åŠ å¸¦ç¼“å­˜çš„MySQLæ•°æ®åº“"""
        cache_config = self._create_cached_config()
        
        response = self.bridge.add_mysql_database(
            alias="mysql_cached",
            host="172.16.0.21",
            port=3306,
            database="testdb",
            username="testdb",
            password="yash2vCiBA&B#h$#i&gb@IGSTh&cP#QC^",
            max_connections=10,
            min_connections=2,
            connection_timeout=30,  # 30ç§’è¿æ¥è¶…æ—¶
            idle_timeout=600,       # 10åˆ†é’Ÿç©ºé—²è¶…æ—¶
            max_lifetime=3600,      # 1å°æ—¶æœ€å¤§ç”Ÿå‘½å‘¨æœŸ
            cache_config=cache_config
        )
        
        result = json.loads(response)
        if not result.get("success"):
            raise Exception(f"æ·»åŠ ç¼“å­˜MySQLæ•°æ®åº“å¤±è´¥: {result.get('error')}")
    
    def _add_non_cached_mysql_database(self):
        """æ·»åŠ ä¸å¸¦ç¼“å­˜çš„MySQLæ•°æ®åº“"""
        # çœŸæ­£çš„æ— ç¼“å­˜é…ç½®ï¼šä¸åˆ›å»ºä»»ä½•ç¼“å­˜ç®¡ç†å™¨
        cache_config = None
        
        response = self.bridge.add_mysql_database(
            alias="mysql_non_cached",
            host="172.16.0.21",
            port=3306,
            database="testdb",
            username="testdb",
            password="yash2vCiBA&B#h$#i&gb@IGSTh&cP#QC^",
            max_connections=10,
            min_connections=2,
            connection_timeout=30,  # 30ç§’è¿æ¥è¶…æ—¶
            idle_timeout=600,       # 10åˆ†é’Ÿç©ºé—²è¶…æ—¶
            max_lifetime=3600,      # 1å°æ—¶æœ€å¤§ç”Ÿå‘½å‘¨æœŸ
            cache_config=cache_config
        )
        
        result = json.loads(response)
        if not result.get("success"):
            raise Exception(f"æ·»åŠ éç¼“å­˜MySQLæ•°æ®åº“å¤±è´¥: {result.get('error')}")
    
    def setup_test_data(self) -> bool:
        """è®¾ç½®æµ‹è¯•æ•°æ®"""
        print("\nğŸ”§ è®¾ç½®MySQLæµ‹è¯•æ•°æ®...")
        
        try:
            max_retries = 3  # æœ€å¤§é‡è¯•æ¬¡æ•°
            operation_timeout = 5  # å•ä¸ªæ“ä½œè¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
            
            # åŸºç¡€æµ‹è¯•ç”¨æˆ·ï¼ˆä¸ºä¸åŒæ•°æ®åº“ä½¿ç”¨ä¸åŒçš„æ•°æ®é¿å…å†²çªï¼‰
            cached_users = [
                TestUser.new(i, f"ç¼“å­˜ç”¨æˆ·{i}", f"cached_user{i}@example.com", 20 + (i % 50), 
                           ["åŒ—äº¬", "ä¸Šæµ·", "å¹¿å·", "æ·±åœ³", "æ­å·"][i % 5])
                for i in range(1, 1001)  # 1000æ¡è®°å½•
            ]
            
            non_cached_users = [
                TestUser.new(i, f"éç¼“å­˜ç”¨æˆ·{i}", f"non_cached_user{i}@example.com", 20 + (i % 50),
                           ["åŒ—äº¬", "ä¸Šæµ·", "å¹¿å·", "æ·±åœ³", "æ­å·"][i % 5])
                for i in range(1, 1001)  # 1000æ¡è®°å½•
            ]
            
            # åˆ›å»ºæµ‹è¯•æ•°æ®åˆ°ç¼“å­˜æ•°æ®åº“
            for i, user in enumerate(cached_users):
                retry_count = 0
                success = False
                
                while retry_count < max_retries and not success:
                    try:
                        start_time = time.time()
                        response = self.bridge.create(self.table_name, user.to_json(), "mysql_cached")
                        
                        # æ£€æŸ¥æ“ä½œæ˜¯å¦è¶…æ—¶
                        if time.time() - start_time > operation_timeout:
                            raise TimeoutError(f"æ“ä½œè¶…æ—¶ï¼ˆ>{operation_timeout}ç§’ï¼‰")
                        
                        result = json.loads(response)
                        if not result.get("success"):
                            raise Exception(result.get('error'))
                        
                        success = True
                        if i == 0:  # åªæ‰“å°ç¬¬ä¸€æ¡è®°å½•çš„ç»“æœ
                            print(f"  âœ… åˆ›å»ºç¼“å­˜ç”¨æˆ·æ•°æ®æˆåŠŸ")
                            
                    except Exception as e:
                        retry_count += 1
                        if retry_count >= max_retries:
                            print(f"âš ï¸ åˆ›å»ºç¼“å­˜ç”¨æˆ·æ•°æ®å¤±è´¥ï¼ˆé‡è¯•{max_retries}æ¬¡åæ”¾å¼ƒï¼‰: {e}")
                            return False
                        else:
                            print(f"âš ï¸ åˆ›å»ºç¼“å­˜ç”¨æˆ·æ•°æ®å¤±è´¥ï¼Œé‡è¯• {retry_count}/{max_retries}: {e}")
                            time.sleep(1)  # é‡è¯•å‰ç­‰å¾…1ç§’
            
            # åˆ›å»ºæµ‹è¯•æ•°æ®åˆ°éç¼“å­˜æ•°æ®åº“
            for i, user in enumerate(non_cached_users):
                retry_count = 0
                success = False
                
                while retry_count < max_retries and not success:
                    try:
                        start_time = time.time()
                        response = self.bridge.create(self.table_name, user.to_json(), "mysql_non_cached")
                        
                        # æ£€æŸ¥æ“ä½œæ˜¯å¦è¶…æ—¶
                        if time.time() - start_time > operation_timeout:
                            raise TimeoutError(f"æ“ä½œè¶…æ—¶ï¼ˆ>{operation_timeout}ç§’ï¼‰")
                        
                        result = json.loads(response)
                        if not result.get("success"):
                            raise Exception(result.get('error'))
                        
                        success = True
                        if i == 0:  # åªæ‰“å°ç¬¬ä¸€æ¡è®°å½•çš„ç»“æœ
                            print(f"  âœ… åˆ›å»ºéç¼“å­˜ç”¨æˆ·æ•°æ®æˆåŠŸ")
                            
                    except Exception as e:
                        retry_count += 1
                        if retry_count >= max_retries:
                            print(f"âš ï¸ åˆ›å»ºéç¼“å­˜ç”¨æˆ·æ•°æ®å¤±è´¥ï¼ˆé‡è¯•{max_retries}æ¬¡åæ”¾å¼ƒï¼‰: {e}")
                            return False
                        else:
                            print(f"âš ï¸ åˆ›å»ºéç¼“å­˜ç”¨æˆ·æ•°æ®å¤±è´¥ï¼Œé‡è¯• {retry_count}/{max_retries}: {e}")
                            time.sleep(1)  # é‡è¯•å‰ç­‰å¾…1ç§’
            
            print(f"  âœ… åˆ›å»ºäº† {len(cached_users) + len(non_cached_users)} æ¡æµ‹è¯•è®°å½•ï¼ˆæ¯ä¸ªæ•°æ®åº“{len(cached_users)}æ¡ï¼‰")
            print(f"  ğŸ“ ä½¿ç”¨è¡¨åç§°: {self.table_name}")
            return True
            
        except Exception as e:
            print(f"âŒ è®¾ç½®æµ‹è¯•æ•°æ®å¤±è´¥: {e}")
            return False
    
    def warmup_cache(self) -> bool:
        """ç¼“å­˜é¢„çƒ­"""
        print("\nğŸ”¥ ç¼“å­˜é¢„çƒ­...")
        
        try:
            # é¢„çƒ­æŸ¥è¯¢1 - ä¸test_query_operationsä¸­çš„æŸ¥è¯¢æ¡ä»¶å®Œå…¨ä¸€è‡´
            query_conditions_1 = json.dumps([
                {"field": "age", "operator": "Gte", "value": 25},
                {"field": "age", "operator": "Lte", "value": 35},
                {"field": "name", "operator": "Contains", "value": "ç”¨æˆ·"},
                {"field": "email", "operator": "Contains", "value": "@example.com"}
            ])
            self.bridge.find(self.table_name, query_conditions_1, "mysql_cached")
            
            # é¢„çƒ­æŸ¥è¯¢2 - ä¸test_repeated_queriesä¸­çš„æŸ¥è¯¢æ¡ä»¶å®Œå…¨ä¸€è‡´
            query_conditions_2 = json.dumps([
                {"field": "city", "operator": "Eq", "value": "åŒ—äº¬"},
                {"field": "age", "operator": "Gte", "value": 30}
            ])
            self.bridge.find(self.table_name, query_conditions_2, "mysql_cached")
            
            # æŒ‰IDæŸ¥è¯¢é¢„çƒ­ - é¢„çƒ­æ‰¹é‡æŸ¥è¯¢ä¸­ä¼šç”¨åˆ°çš„ID
            for i in range(1, 21):
                query_conditions = json.dumps([
                    {"field": "id", "operator": "Eq", "value": i}
                ])
                self.bridge.find(self.table_name, query_conditions, "mysql_cached")
            
            # é¢„çƒ­å¹´é¾„æŸ¥è¯¢ - é¢„çƒ­æ‰¹é‡æŸ¥è¯¢ä¸­çš„å¹´é¾„æŸ¥è¯¢
            for i in range(1, 11):
                age_conditions = json.dumps([
                    {"field": "age", "operator": "Eq", "value": 20 + (i % 50)}
                ])
                self.bridge.find(self.table_name, age_conditions, "mysql_cached")
            
            print("  âœ… ç¼“å­˜é¢„çƒ­å®Œæˆï¼Œé¢„çƒ­äº†æ‰€æœ‰æµ‹è¯•æŸ¥è¯¢æ¨¡å¼")
            print("  ğŸ“Š é¢„çƒ­å†…å®¹: 2ç§å¤æ‚æŸ¥è¯¢ + 20æ¡IDæŸ¥è¯¢ + 10ç§å¹´é¾„æŸ¥è¯¢")
            return True
            
        except Exception as e:
            print(f"âŒ ç¼“å­˜é¢„çƒ­å¤±è´¥: {e}")
            return False
    
    def test_query_operations(self) -> bool:
        """æµ‹è¯•æŸ¥è¯¢æ“ä½œæ€§èƒ½"""
        print("\nğŸ” æµ‹è¯•MySQLæŸ¥è¯¢æ“ä½œæ€§èƒ½...")
        
        try:
            # æ„å»ºå¤æ‚æŸ¥è¯¢æ¡ä»¶ - æŸ¥æ‰¾ç‰¹å®šç”¨æˆ·ä¸”å¹´é¾„ç¬¦åˆæ¡ä»¶
            query_conditions = json.dumps([
                {"field": "age", "operator": "Gte", "value": 25},
                {"field": "age", "operator": "Lte", "value": 35},
                {"field": "name", "operator": "Contains", "value": "ç”¨æˆ·"},
                {"field": "email", "operator": "Contains", "value": "@example.com"}
            ])
            
            # æµ‹è¯•ç¼“å­˜æ•°æ®åº“æŸ¥è¯¢ï¼ˆ100æ¬¡ï¼Œå¢åŠ é‡å¤æŸ¥è¯¢ä»¥ä½“ç°ç¼“å­˜ä¼˜åŠ¿ï¼‰
            start_time = time.time()
            for i in range(1, 101):
                self.bridge.find(self.table_name, query_conditions, "mysql_cached")
            cached_duration = (time.time() - start_time) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
            
            # æµ‹è¯•éç¼“å­˜æ•°æ®åº“æŸ¥è¯¢ï¼ˆ100æ¬¡ï¼‰
            start_time = time.time()
            for i in range(1, 101):
                self.bridge.find(self.table_name, query_conditions, "mysql_non_cached")
            non_cached_duration = (time.time() - start_time) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
            
            # è®°å½•ç»“æœ
            result = PerformanceResult.new(
                "å¤æ‚æŸ¥è¯¢æ“ä½œ (100æ¬¡)",
                cached_duration,
                non_cached_duration
            )
            self.results.append(result)
            
            print(f"  âœ… æŸ¥è¯¢æ“ä½œæµ‹è¯•å®Œæˆ")
            print(f"  ğŸ“Š ç¼“å­˜ç‰ˆæœ¬: {cached_duration:.1f}ms, éç¼“å­˜ç‰ˆæœ¬: {non_cached_duration:.1f}ms")
            print(f"  ğŸš€ æ€§èƒ½æå‡: {result.improvement_ratio:.1f}å€")
            return True
            
        except Exception as e:
            print(f"âŒ æŸ¥è¯¢æ“ä½œæµ‹è¯•å¤±è´¥: {e}")
            return False
    
    def test_repeated_queries(self) -> bool:
        """æµ‹è¯•é‡å¤æŸ¥è¯¢æ€§èƒ½ï¼ˆç¼“å­˜å‘½ä¸­ï¼‰"""
        print("\nğŸ”„ æµ‹è¯•MySQLé‡å¤æŸ¥è¯¢æ€§èƒ½...")
        
        try:
            # æ„å»ºé‡å¤æŸ¥è¯¢æ¡ä»¶ - æŸ¥æ‰¾åŒ—äº¬ä¸”å¹´é¾„>=30çš„ç”¨æˆ·
            query_conditions = json.dumps([
                {"field": "city", "operator": "Eq", "value": "åŒ—äº¬"},
                {"field": "age", "operator": "Gte", "value": 30}
            ])
            
            # æµ‹è¯•ç¼“å­˜æ•°æ®åº“é‡å¤æŸ¥è¯¢ï¼ˆ500æ¬¡ï¼Œå¤§é‡é‡å¤æŸ¥è¯¢ä½“ç°ç¼“å­˜ä¼˜åŠ¿ï¼‰
            start_time = time.time()
            for i in range(1, 501):
                self.bridge.find(self.table_name, query_conditions, "mysql_cached")
            cached_duration = (time.time() - start_time) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
            
            # æµ‹è¯•éç¼“å­˜æ•°æ®åº“é‡å¤æŸ¥è¯¢ï¼ˆ500æ¬¡ï¼‰
            start_time = time.time()
            for i in range(1, 501):
                self.bridge.find(self.table_name, query_conditions, "mysql_non_cached")
            non_cached_duration = (time.time() - start_time) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
            
            # è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯
            try:
                cache_stats_response = self.bridge.get_cache_stats("mysql_cached")
                cache_stats = json.loads(cache_stats_response)
                if cache_stats.get("success"):
                    stats = cache_stats.get("stats", {})
                    hit_rate = stats.get("hit_rate", 0.0) * 100
                    print(f"  ğŸ“ˆ ç¼“å­˜ç»Ÿè®¡ - å‘½ä¸­: {stats.get('hits', 0)}, æœªå‘½ä¸­: {stats.get('misses', 0)}, å‘½ä¸­ç‡: {hit_rate:.1f}%")
                else:
                    hit_rate = None
                    print(f"  âš ï¸ è·å–ç¼“å­˜ç»Ÿè®¡å¤±è´¥: {cache_stats.get('error')}")
            except Exception as e:
                hit_rate = None
                print(f"  âš ï¸ è·å–ç¼“å­˜ç»Ÿè®¡å¼‚å¸¸: {e}")
            
            # è®°å½•ç»“æœ
            result = PerformanceResult.new(
                "é‡å¤æŸ¥è¯¢ (500æ¬¡)",
                cached_duration,
                non_cached_duration
            )
            if hit_rate is not None:
                result = result.with_cache_hit_rate(hit_rate)
            self.results.append(result)
            
            print(f"  âœ… é‡å¤æŸ¥è¯¢æµ‹è¯•å®Œæˆ")
            print(f"  ğŸ“Š ç¼“å­˜ç‰ˆæœ¬: {cached_duration:.1f}ms, éç¼“å­˜ç‰ˆæœ¬: {non_cached_duration:.1f}ms")
            print(f"  ğŸš€ æ€§èƒ½æå‡: {result.improvement_ratio:.1f}å€")
            return True
            
        except Exception as e:
            print(f"âŒ é‡å¤æŸ¥è¯¢æµ‹è¯•å¤±è´¥: {e}")
            return False
    
    def test_batch_queries(self) -> bool:
        """æµ‹è¯•æ‰¹é‡æŸ¥è¯¢æ€§èƒ½"""
        print("\nğŸ“¦ æµ‹è¯•MySQLæ‰¹é‡æŸ¥è¯¢æ€§èƒ½...")
        
        try:
            # æµ‹è¯•ç¼“å­˜æ•°æ®åº“çš„æ‰¹é‡æŸ¥è¯¢ï¼ˆæ··åˆIDæŸ¥è¯¢å’ŒèŒƒå›´æŸ¥è¯¢ï¼‰
            start_time = time.time()
            # å…ˆæŸ¥è¯¢ä¸€äº›å…·ä½“IDï¼ˆè¿™äº›åº”è¯¥å‘½ä¸­ç¼“å­˜ï¼‰
            for i in range(1, 21):
                id_conditions = json.dumps([
                    {"field": "id", "operator": "Eq", "value": i}
                ])
                self.bridge.find(self.table_name, id_conditions, "mysql_cached")
            
            # å†æŸ¥è¯¢ä¸€äº›å¹´é¾„èŒƒå›´
            for i in range(1, 21):
                age_conditions = json.dumps([
                    {"field": "age", "operator": "Eq", "value": 20 + (i % 50)}
                ])
                self.bridge.find(self.table_name, age_conditions, "mysql_cached")
            cached_duration = (time.time() - start_time) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
            
            # æµ‹è¯•éç¼“å­˜æ•°æ®åº“çš„æ‰¹é‡æŸ¥è¯¢ï¼ˆç›¸åŒçš„æŸ¥è¯¢æ¨¡å¼ï¼‰
            start_time = time.time()
            # æŸ¥è¯¢ç›¸åŒçš„ID
            for i in range(1, 21):
                id_conditions = json.dumps([
                    {"field": "id", "operator": "Eq", "value": i}
                ])
                self.bridge.find(self.table_name, id_conditions, "mysql_non_cached")
            
            # æŸ¥è¯¢ç›¸åŒçš„å¹´é¾„èŒƒå›´
            for i in range(1, 21):
                age_conditions = json.dumps([
                    {"field": "age", "operator": "Eq", "value": 20 + (i % 50)}
                ])
                self.bridge.find(self.table_name, age_conditions, "mysql_non_cached")
            non_cached_duration = (time.time() - start_time) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
            
            # è®°å½•ç»“æœ
            result = PerformanceResult.new(
                "æ‰¹é‡æŸ¥è¯¢ (20æ¬¡IDæŸ¥è¯¢ + 20æ¬¡å¹´é¾„æŸ¥è¯¢)",
                cached_duration,
                non_cached_duration
            )
            self.results.append(result)
            
            print(f"  âœ… æ‰¹é‡æŸ¥è¯¢æµ‹è¯•å®Œæˆ")
            print(f"  ğŸ“Š ç¼“å­˜ç‰ˆæœ¬: {cached_duration:.1f}ms, éç¼“å­˜ç‰ˆæœ¬: {non_cached_duration:.1f}ms")
            print(f"  ğŸš€ æ€§èƒ½æå‡: {result.improvement_ratio:.1f}å€")
            return True
            
        except Exception as e:
            print(f"âŒ æ‰¹é‡æŸ¥è¯¢æµ‹è¯•å¤±è´¥: {e}")
            return False
    
    def test_update_operations(self) -> bool:
        """æµ‹è¯•æ›´æ–°æ“ä½œæ€§èƒ½"""
        print("\nâœï¸ æµ‹è¯•MySQLæ›´æ–°æ“ä½œæ€§èƒ½...")
        
        try:
            # æ„å»ºæ›´æ–°æ•°æ®
            update_data = json.dumps({"age": 30})
            
            # æµ‹è¯•ç¼“å­˜æ•°æ®åº“çš„æ›´æ–°æ“ä½œ
            start_time = time.time()
            for i in range(1, 21):
                update_conditions = json.dumps([
                    {"field": "id", "operator": "Eq", "value": i}
                ])
                self.bridge.update(self.table_name, update_conditions, update_data, "mysql_cached")
            cached_duration = (time.time() - start_time) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
            
            # æµ‹è¯•éç¼“å­˜æ•°æ®åº“çš„æ›´æ–°æ“ä½œ
            start_time = time.time()
            for i in range(1, 21):
                update_conditions = json.dumps([
                    {"field": "id", "operator": "Eq", "value": i}
                ])
                self.bridge.update(self.table_name, update_conditions, update_data, "mysql_non_cached")
            non_cached_duration = (time.time() - start_time) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
            
            # è®°å½•ç»“æœ
            result = PerformanceResult.new(
                "æ›´æ–°æ“ä½œ (20æ¬¡)",
                cached_duration,
                non_cached_duration
            )
            self.results.append(result)
            
            print(f"  âœ… æ›´æ–°æ“ä½œæµ‹è¯•å®Œæˆ")
            print(f"  ğŸ“Š ç¼“å­˜ç‰ˆæœ¬: {cached_duration:.1f}ms, éç¼“å­˜ç‰ˆæœ¬: {non_cached_duration:.1f}ms")
            print(f"  ğŸš€ æ€§èƒ½æå‡: {result.improvement_ratio:.1f}å€")
            return True
            
        except Exception as e:
            print(f"âŒ æ›´æ–°æ“ä½œæµ‹è¯•å¤±è´¥: {e}")
            return False
    
    def display_results(self):
        """æ˜¾ç¤ºæµ‹è¯•ç»“æœ"""
        print("\n" + "="*60)
        print("ğŸ¯ MySQLç¼“å­˜æ€§èƒ½æµ‹è¯•ç»“æœæ±‡æ€»")
        print("="*60)
        
        total_cached_time = 0.0
        total_non_cached_time = 0.0
        
        for i, result in enumerate(self.results, 1):
            total_cached_time += result.with_cache
            total_non_cached_time += result.without_cache
            
            print(f"\nğŸ“Š æµ‹è¯• {i}: {result.operation}")
            print(f"   ğŸŸ¢ å¸¦ç¼“å­˜:   {result.with_cache:.1f} ms")
            print(f"   ğŸ”´ ä¸å¸¦ç¼“å­˜: {result.without_cache:.1f} ms")
            print(f"   ğŸš€ æ€§èƒ½æå‡: {result.improvement_ratio:.1f}å€")
            
            if result.cache_hit_rate is not None:
                print(f"   ğŸ“ˆ ç¼“å­˜å‘½ä¸­ç‡: {result.cache_hit_rate:.1f}%")
        
        # æ€»ä½“ç»Ÿè®¡
        print(f"\n" + "="*60)
        print("ğŸ“ˆ æ€»ä½“æ€§èƒ½ç»Ÿè®¡")
        print("="*60)
        print(f"ğŸŸ¢ æ€»ç¼“å­˜æ—¶é—´:   {total_cached_time:.1f} ms")
        print(f"ğŸ”´ æ€»éç¼“å­˜æ—¶é—´: {total_non_cached_time:.1f} ms")
        
        if total_cached_time > 0:
            overall_improvement = total_non_cached_time / total_cached_time
            print(f"ğŸš€ æ€»ä½“æ€§èƒ½æå‡: {overall_improvement:.1f}å€")
            
            time_saved = total_non_cached_time - total_cached_time
            time_saved_percent = (time_saved / total_non_cached_time) * 100
            print(f"â±ï¸ èŠ‚çœæ—¶é—´: {time_saved:.1f} ms ({time_saved_percent:.1f}%)")
        
        print("="*60)
    
    def run_performance_test(self) -> bool:
        """è¿è¡Œå®Œæ•´çš„æ€§èƒ½æµ‹è¯•"""
        print("ğŸš€ å¼€å§‹MySQLç¼“å­˜æ€§èƒ½å¯¹æ¯”æµ‹è¯•")
        print(f"ğŸ“ RAT QuickDB ç‰ˆæœ¬: {get_version()}")
        print(f"ğŸ“‹ æµ‹è¯•ä¿¡æ¯: {get_info()}")
        
        try:
            # åˆå§‹åŒ–æµ‹è¯•ç¯å¢ƒ
            if not self.initialize():
                return False
            
            # è®¾ç½®æµ‹è¯•æ•°æ®
            if not self.setup_test_data():
                return False
            
            # é¢„çƒ­ç¼“å­˜
            if not self.warmup_cache():
                return False
            
            # è¿è¡Œå„ç§æ€§èƒ½æµ‹è¯•
            if not self.test_query_operations():
                return False
            
            if not self.test_repeated_queries():
                return False
            
            if not self.test_batch_queries():
                return False
            
            if not self.test_update_operations():
                return False
            
            # æ˜¾ç¤ºç»“æœ
            self.display_results()
            
            print("\nğŸ‰ MySQLç¼“å­˜æ€§èƒ½æµ‹è¯•å®Œæˆï¼")
            return True
            
        except Exception as e:
            print(f"âŒ æ€§èƒ½æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            return False
    
    def cleanup_resources(self):
        """æ¸…ç†æµ‹è¯•æ–‡ä»¶å’Œæ•°æ®ï¼ˆå®ç° GracefulShutdownMixin çš„æŠ½è±¡æ–¹æ³•ï¼‰"""
        print("ğŸ§¹ æ¸…ç† MySQL æµ‹è¯•æ•°æ®...")
        
        try:
            # æ¸…ç†æµ‹è¯•è¡¨æ•°æ®ï¼Œæ·»åŠ è¶…æ—¶é™åˆ¶
            if self.bridge:
                try:
                    # è®¾ç½®æ¸…ç†æ“ä½œçš„è¶…æ—¶æ—¶é—´
                    cleanup_start = time.time()
                    cleanup_timeout = 5  # 5ç§’è¶…æ—¶
                    
                    # åˆ é™¤ç¼“å­˜æ•°æ®åº“ä¸­çš„æµ‹è¯•æ•°æ®
                    if time.time() - cleanup_start < cleanup_timeout:
                        delete_conditions = json.dumps([
                            {"field": "id", "operator": "Contains", "value": "cached_user_"}
                        ])
                        self.bridge.delete(self.table_name, delete_conditions, "mysql_cached")
                    
                    # åˆ é™¤éç¼“å­˜æ•°æ®åº“ä¸­çš„æµ‹è¯•æ•°æ®
                    if time.time() - cleanup_start < cleanup_timeout:
                        delete_conditions = json.dumps([
                            {"field": "id", "operator": "Contains", "value": "non_cached_user_"}
                        ])
                        self.bridge.delete(self.table_name, delete_conditions, "mysql_non_cached")
                    
                    print(f"  âœ… å·²æ¸…ç†MySQLæµ‹è¯•è¡¨: {self.table_name}")
                except Exception as e:
                    print(f"  âš ï¸  æ¸…ç†MySQLæµ‹è¯•æ•°æ®å¤±è´¥: {e}")
            
            print("âœ… MySQL æµ‹è¯•æ•°æ®æ¸…ç†å®Œæˆ")
            
        except Exception as e:
            print(f"âŒ æ¸…ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
    
    def cleanup(self):
        """å…¼å®¹æ€§æ–¹æ³•ï¼Œè°ƒç”¨ä¼˜é›…å…³é—­"""
        self.shutdown()


def main():
    """ä¸»å‡½æ•°"""
    # æ³¨å†Œä¿¡å·å¤„ç†å™¨
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)
    
    global test_instance
    test_instance = MySqlCachePerformanceTest()
    
    try:
        success = test_instance.run_performance_test()
        if success:
            print("âœ… æµ‹è¯•æˆåŠŸå®Œæˆ")
            exit_code = 0
        else:
            print("âŒ æµ‹è¯•å¤±è´¥")
            exit_code = 1
    except KeyboardInterrupt:
        print("\nğŸ›‘ ç”¨æˆ·ä¸­æ–­æµ‹è¯•")
        exit_code = 130
    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿæœªé¢„æœŸé”™è¯¯: {e}")
        exit_code = 1
    finally:
        # æ¸…ç†èµ„æº
        if test_instance:
            test_instance.shutdown()
    
    exit(exit_code)


if __name__ == "__main__":
    main()